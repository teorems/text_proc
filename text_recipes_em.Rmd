---
output: html_document
editor_options: 
  chunk_output_type: inline
---


```{r}
library(readr)
rr <- readr::read_tsv("Restaurant_Reviews.tsv", col_types = "cf")

```


# Objectif du projet

A partir du dataset `Restaurant_Reviews.tsv`, qui contient `nrow(rr)` critiques de restaurants, [source] labellisées en 2 catégories, 0 (négative) et 1 (positive), nous allons montrer comment développer, en utilisant le framework *Tidymodels*, un modèle de ML (classifier) qui permet de prédire la catégorie des nouvelles données.

```{r}
head(rr)
```


# Définition d'un modèle

```{r}

#A ajouter : analyse du sentiment
# Load required libraries
library(tidymodels)
library(textrecipes)


# Split the data into training and testing sets
set.seed(123)
split <- initial_split(rr, prop = 0.7)
train_data <- training(split)
test_data <- testing(split)

```

## One model

Pour démontrer le procédé standard ainsi que la syntaxe typique de tidymodels on va développer un simple modèle de régression logistique que après la **tokenization** (décomposition des textes en mots) et le calcul du **tfidf** (indicateur qui permet de mesurer l'importance d'un mot dans une phrase par rapport à la fréquence de ce mot dans l'ensemble de textes ) :


```{r model_illustration}

# création d'une "recipe" (spécification des variables et preprocessing )
recipe <- recipe(Liked ~ Review, data = train_data) %>%
  step_tokenize(Review) %>%
  step_tokenfilter(Review) %>%
  step_tfidf(Review) 

recipe

# viz du df resultant après transformations
recipe |> prep() |> bake(new_data = NULL)

# modèle de base de régression logistique
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_reg

# création d'un workflow (modèle + rècipe)
workflow <- workflow() %>%
  add_model(log_reg) %>%
  add_recipe(recipe)

workflow

# entrainement sur le train_data
model <- workflow %>% fit(train_data)

# predictions sur les test set
predictions <- predict(model, new_data = test_data, type = "prob") |> bind_cols(predict(model, new_data = test_data, type = "class"))

# resultats
res <- bind_cols(predictions, test_data |>  select(Liked))
head(res)

```

## Metrics

```{r}
# matrice de confusion
conf_matrix <- conf_mat(res, truth = Liked, estimate = .pred_class)
conf_matrix

# ensemble de metrics categoriques derivées de conf_mat
conf_matrix |> summary()

# Print the confusion matrix
autoplot(conf_matrix, type ="heatmap")

# mise en place d'un indicateur qui contient de metrics
class_metrics <- metric_set(roc_auc, accuracy, kap)

# Calcul de metrics - parametre de prob en +
res |> class_metrics(estimate = .pred_class, truth = Liked, .pred_1)

# roc curve
res |> roc_curve(truth = Liked, .pred_1) |> autoplot()

```

# Plusieurs modèles

On va comparer trois modèle differents : le modèle déjà utilisé de regression logistique, un random forest et un modele de boosting. Le modèles et le procédés de tuning sont 

```{r wf_map}

# mise en place du parallélisme (utilisation de plusieurs proces pour )
library(doParallel)
registerDoParallel()

# model specs
rf <-  rand_forest(mode = "classification", mtry = tune(), trees = 1000, min_n = tune()) 
xgb <- boost_tree(mode = "classification", tree_depth = tune(), learn_rate = tune(), trees = 1000)
ws <- workflow_set(preproc = list(recipe), models = list(log_reg, rf, xgb))

ws

# creation de 1O partition à partir des données d’entraînement
set.seed(123)
rr_cv <- vfold_cv(train_data)

rr_cv

# paramètre pour garder les prédictions et les modèles
library(finetune)
race_cont <- control_race(save_pred = TRUE, save_workflow = TRUE)

# application des modelés aux partitions obtenus avec la cross-validation et réglage des hyperparameters (~15min.)

res <- readRDS("res_race_anova_3_mods.Rds")

# system.time({
# res <- workflow_map(ws, resamples = rr_cv, "tune_race_anova", control = race_cont)
# })

stopImplicitCluster()

```

# Evaluation et comparaion

```{r}
# metrics
res |> collect_metrics()

res |> rank_results("roc_auc") |> 
  filter(.metric == "roc_auc")

autoplot(res,
         rank_metric = "roc_auc",
         metric = "roc_auc"
         ) +
  
   geom_text(aes(y = mean - 0.03, label = wflow_id), angle = 90, hjust = 1) +
   theme(legend.position = "none") + ylim(c(0.6,0.9))

# plot detaillé d'un modèle specifique
autoplot(res, id = "recipe_boost_tree")

```

# Choix du modèle

```{r}
best_results <- 
   res %>% 
   extract_workflow_set_result("recipe_rand_forest") %>% 
   select_best(metric = "roc_auc")
# best boosting model
best_results

best_results_last_fit <- 
   res %>% 
   extract_workflow("recipe_rand_forest") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = split)

collect_metrics(best_results_last_fit)
```

# Metrics & Roc Curve 

```{r}

#roc auc & other metrics
best_results_last_fit
fin_preds <- best_results_last_fit |> collect_predictions() |> select(.pred_1, .pred_class, Liked) 
fin_preds
fin_preds|> roc_curve(.pred_1, truth = Liked) |> autoplot()
fin_preds |> class_metrics(estimate = .pred_class, truth = Liked, .pred_1)

# confusion matrix & cat metrics
(cm <- fin_preds|> conf_mat(.pred_class, truth = Liked))
cm |> summary()
autoplot(cm, type ="heatmap")
```


# Text embeddings

```{r}
library(text)
# !! operation assez longue !!
#rr_emb <- textEmbed(rr)
# Completed layers output for Review (variable: 1/1, duration: 4.267763 mins).
# Completed layers aggregation for word_type_embeddings. 
# Completed layers aggregation (variable 1/1, duration: 7.010227 mins).
# Completed layers aggregation (variable 1/1, duration: 7.108091 mins).
rr_emb <- readRDS("rr_embeddings_bert_uncased.Rds")
```

# PCA et projection sur les axes

```{r}
res_pca <- prcomp(rr_emb$texts$Review)

summary(res_pca)

X <- tibble(as.data.frame(res_pca$x)) |> 
  bind_cols(review = rr$Review, liked = rr$Liked) |> 
  mutate(id = row_number()) |> 
  select(id, review, liked, everything())

p <- ggplot(X, aes(PC4, PC5)) + geom_point(aes(text = paste(id,"-", review)), color = if_else(rr$Liked==1, 'blue', 'green'))

library(plotly)

ggplotly(p, tooltip = "text")
```



